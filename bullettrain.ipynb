{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a23ebdc-b299-4f0d-a918-3f74613c6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shinkansen Travel Experience\n",
    "#The goal of the problem is to predict whether a passenger was satisfied or not considering his/her overall experience of traveling on the Shinkansen Bullet Train.\n",
    "\n",
    "#Dataset: \n",
    "#The problem consists of 2 separate datasets: Travel data & Survey data. Travel data has information related to passengers and attributes related to the Shinkansen train, in which they traveled. The survey data is aggregated data of surveys indicating the post-service experience. You are expected to treat both these datasets as raw data and perform any necessary data cleaning/validation steps as required.\n",
    "\n",
    "#The data has been split into two groups and provided in the Dataset folder. The folder contains both train and test data separately.\n",
    "\n",
    "#Train_Data\n",
    "#Test_Data\n",
    "\n",
    "#Target Variable: Overall_Experience (1 represents ‘satisfied’, and 0 represents ‘not satisfied’)\n",
    "\n",
    "#The training set can be used to build your machine-learning model. The training set has labels for the target column - Overall_Experience.\n",
    "\n",
    "#The testing set should be used to see how well your model performs on unseen data. For the test set, it is expected to predict the ‘Overall_Experience’ level for each participant.\n",
    "\n",
    "#Data Dictionary:\n",
    "#All the data is self-explanatory. The survey levels are explained in the Data Dictionary file.\n",
    "\n",
    "#Submission File Format: You will need to submit a CSV file with exactly 35,602 entries plus a header row. The file should have exactly two columns\n",
    "\n",
    "#ID\n",
    "#Overall_Experience (contains 0 & 1 values, 1 represents ‘Satisfied’, and 0 represents ‘Not Satisfied’)\n",
    "\n",
    "#Evaluation Criteria:\n",
    "\n",
    "#Accuracy Score: The evaluation metric is simply the percentage of predictions made by the model that turned out to be correct. This is also called the accuracy of the model. It will be calculated as the total number of correct predictions (True Positives + True Negatives) divided by the total number of observations in the dataset.\n",
    " \n",
    "#In other words, the best possible accuracy is 100% (or 1), and the worst possible accuracy is 0%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4b50a29-660e-436a-ad35-92bda882ee21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: xgboost in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bigba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\bigba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\bigba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~etuptools (C:\\Users\\bigba\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn matplotlib seaborn xgboost openpyxl \n",
    "\n",
    "# Explanation:\n",
    "#Pandas will be used to load and preprocess your CSV files.\n",
    "#Scikit-learn will allow you to train a machine learning model (e.g., RandomForest, XGBoost, etc.).\n",
    "#Matplotlib and Seaborn are for visualizing your data and the model performance.\n",
    "#XGBoost (or LightGBM) is useful for training high-performance models, especially if you have large datasets.\n",
    "#Openpyxl is for working with Excel files, such as reading the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93717558-7117-43cf-89cd-0824ba606b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47a7b64-e94a-4ad1-91b3-7c0e212627fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "\n",
    "# Load the datasets\n",
    "travel_train = pd.read_csv('Traveldata_train.csv')\n",
    "survey_train = pd.read_csv('Surveydata_train.csv')\n",
    "\n",
    "travel_test = pd.read_csv('Traveldata_test.csv')\n",
    "survey_test = pd.read_csv('Surveydata_test.csv')\n",
    "\n",
    "# Merge Travel and Survey data based on ID\n",
    "train_data = pd.merge(travel_train, survey_train, on='ID', how='inner')\n",
    "test_data = pd.merge(travel_test, survey_test, on='ID', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a49e16f4-bdb0-42fe-8060-636fb15fa0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observations:\n",
    "\n",
    "#Training Travel Data Summary:\n",
    "#The Travel data includes information related to the passengers and their travel details on the Shinkansen Bullet Train.\n",
    "\n",
    "#Columns:\n",
    "\n",
    "#ID: Unique identifier for each passenger (numeric).\n",
    "#Gender: Gender of the passenger (categorical: Male, Female).\n",
    "#Customer_Type: Type of customer (categorical: Loyal Customer).\n",
    "#Age: Age of the passenger (numeric).\n",
    "#Type_Travel: Type of travel (categorical: Business Travel, Personal Travel, Business Travel).\n",
    "#Travel_Class: The class of travel (categorical: Business, Eco).\n",
    "#Travel_Distance: Distance traveled by the passenger (numeric, in kilometers).\n",
    "#Departure_Delay_in_Mins: Delay at departure in minutes (numeric).\n",
    "#Arrival_Delay_in_Mins: Delay at arrival in minutes (numeric).\n",
    "#Data Insights:\n",
    "\n",
    "#The Age column ranges from 43 to 52 years, indicating an adult demographic.\n",
    "#The Travel_Distance ranges from 2200 km to 272 km, indicating passengers on both long and short-distance trips.\n",
    "#Departure_Delay_in_Mins and Arrival_Delay_in_Mins indicate varying levels of delays, with some passengers experiencing long delays (e.g., 77 minutes departure delay and 119 minutes arrival delay for one record).\n",
    "#The Travel_Class column has two primary categories: Business and Eco, indicating different class types available.\n",
    "#Training Survey Data Summary:\n",
    "#The Survey data contains post-service experience feedback, which includes the passengers' evaluation of various aspects of the service they experienced.\n",
    "\n",
    "#Columns:\n",
    "\n",
    "#ID: Unique identifier for each passenger (numeric).\n",
    "#Overall_Experience: Overall experience rating (binary: 1 for 'Satisfied', 0 for 'Not Satisfied').\n",
    "#Seat_Comfort: Rating of seat comfort (categorical: Needs Improvement, Poor, Acceptable).\n",
    "#Seat_Class: The class of seat (categorical: Green Car, Ordinary).\n",
    "#Arrival_Time_Convenient: Evaluation of arrival time convenience (categorical: Excellent, Needs Improvement, Acceptable).\n",
    "#Catering: Catering service evaluation (categorical: Excellent, Poor, Acceptable).\n",
    "#Platform_Location: Evaluation of platform location convenience (categorical: Very Convenient, Needs Improvement, Manageable).\n",
    "#Onboard_Wifi_Service: Evaluation of onboard Wi-Fi service (categorical: Good, Needs Improvement, Acceptable).\n",
    "#Onboard_Entertainment: Evaluation of onboard entertainment (categorical: Good, Needs Improvement, Acceptable).\n",
    "#Online_Support: Evaluation of online support (categorical: Excellent, Good, Acceptable).\n",
    "#Ease_of_Online_Booking: Evaluation of online booking ease (categorical: Excellent, Good, Needs Improvement).\n",
    "#Onboard_Service: Evaluation of onboard service (categorical: Excellent, Acceptable, Good).\n",
    "#Legroom: Evaluation of legroom (categorical: Excellent, Acceptable, Needs Improvement).\n",
    "#Baggage_Handling: Evaluation of baggage handling (categorical: Excellent, Good, Needs Improvement, Poor).\n",
    "#CheckIn_Service: Evaluation of check-in service (categorical: Excellent, Needs Improvement, Good).\n",
    "#Cleanliness: Evaluation of cleanliness (categorical: Excellent, Good, Needs Improvement).\n",
    "#Online_Boarding: Evaluation of online boarding experience (categorical: Excellent, Good, Poor, Acceptable).\n",
    "#Data Insights:\n",
    "\n",
    "#Overall_Experience has a binary value of 0 (Not Satisfied) or 1 (Satisfied), which will be the target variable for the model.\n",
    "#A mix of categories appears across various feedback columns. For example:\n",
    "#Seat_Comfort has values like Needs Improvement, Poor, and Acceptable.\n",
    "#Catering ratings range from Excellent to Poor.\n",
    "#Other columns like Legroom, Baggage_Handling, and Cleanliness provide insights into the quality of services.\n",
    "#Online support and Onboard Wi-Fi Service are key factors affecting the satisfaction score, with many records reporting \"Needs Improvement\" or \"Acceptable\" evaluations.\n",
    "#Key Observations:\n",
    "#Target Variable (Overall_Experience):\n",
    "\n",
    "#The target variable in the Survey Data indicates whether the passenger was satisfied (1) or not (0). This is what we will predict based on the available features.\n",
    "#Demographics and Service Features:\n",
    "\n",
    "#The Travel Data gives us insights into the demographic profile (age, gender, travel type) and travel-related features (distance, delays).\n",
    "# Survey Data captures the feedback on various aspects of the service (comfort, Wi-Fi, cleanliness, etc.).\n",
    "#It is expected that factors like catering, seat comfort, online booking, and wifi service might strongly influence satisfaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7561f0d3-f1c6-4c7d-aca0-7852f0ee0422",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Overall_Experience'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Fill missing values for numeric columns with mean\u001b[39;00m\n\u001b[0;32m      8\u001b[0m train_data[numeric_cols] \u001b[38;5;241m=\u001b[39m train_data[numeric_cols]\u001b[38;5;241m.\u001b[39mfillna(train_data[numeric_cols]\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m----> 9\u001b[0m test_data[numeric_cols] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnumeric_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mfillna(test_data[numeric_cols]\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Fill missing values for categorical columns with the mode (most frequent value)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m train_data[categorical_cols] \u001b[38;5;241m=\u001b[39m train_data[categorical_cols]\u001b[38;5;241m.\u001b[39mfillna(train_data[categorical_cols]\u001b[38;5;241m.\u001b[39mmode()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Overall_Experience'] not in index\""
     ]
    }
   ],
   "source": [
    "# Data Preprocessing: Handle Missing Values\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = train_data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = train_data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Fill missing values for numeric columns with mean\n",
    "train_data[numeric_cols] = train_data[numeric_cols].fillna(train_data[numeric_cols].mean())\n",
    "test_data[numeric_cols] = test_data[numeric_cols].fillna(test_data[numeric_cols].mean())\n",
    "\n",
    "# Fill missing values for categorical columns with the mode (most frequent value)\n",
    "train_data[categorical_cols] = train_data[categorical_cols].fillna(train_data[categorical_cols].mode().iloc[0])\n",
    "test_data[categorical_cols] = test_data[categorical_cols].fillna(test_data[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Encoding categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "for col in categorical_cols:\n",
    "    train_data[col] = label_encoder.fit_transform(train_data[col])\n",
    "    test_data[col] = label_encoder.transform(test_data[col])\n",
    "\n",
    "# Separate features (X) and target (y) in the training set\n",
    "X_train = train_data.drop(columns=['ID', 'Overall_Experience'])  # Drop target column in training\n",
    "y_train = train_data['Overall_Experience']  # Target column in training\n",
    "\n",
    "# Separate features (X) in the test set (do not drop 'Overall_Experience' because it's not present in the test set)\n",
    "X_test = test_data.drop(columns=['ID'])  # Only drop 'ID' column in test set\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest or XGBoost model (XGBoost recommended for better performance)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Generate a DataFrame for submission with required format\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],  # Ensure ID column is present from the test set\n",
    "    'Overall_Experience': y_pred\n",
    "})\n",
    "\n",
    "# Save the predictions as a CSV file with the correct format\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Submission file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d2dcf-a1eb-4915-9615-44b07ebbc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying scaling to travel data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# List of numerical columns in Travel Data\n",
    "numerical_columns_train = ['Age', 'Travel_Distance', 'Departure_Delay_in_Mins', 'Arrival_Delay_in_Mins']\n",
    "\n",
    "# Apply scaling to numerical columns in Travel Data\n",
    "train_data[numerical_columns_train] = scaler.fit_transform(train_data[numerical_columns_train])\n",
    "\n",
    "# Check the scaled data in Travel Data\n",
    "print(\"Scaled Travel Data:\")\n",
    "print(train_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d20c91-f470-4eaf-ac64-50ae04923a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying scaling to survey data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# List of numerical columns in Survey Data (excluding 'ID' and 'Overall_Experience')\n",
    "numerical_columns_survey = ['Seat_Comfort', 'Seat_Class', 'Arrival_Time_Convenient', 'Catering']  # Add other numerical feedback columns if needed\n",
    "\n",
    "# Apply scaling to these numerical columns in Survey Data\n",
    "survey_train[numerical_columns_survey] = scaler.fit_transform(survey_train[numerical_columns_survey])\n",
    "\n",
    "# Check the scaled data in Survey Data\n",
    "print(\"Scaled Survey Data:\")\n",
    "print(survey_train.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce818c-0f75-42f9-87be-839c56abc03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the Travel and Survey Data\n",
    "\n",
    "# Merge Travel and Survey data on 'ID'\n",
    "train_merged = pd.merge(train_data, survey_train, on='ID')\n",
    "\n",
    "# Drop one of the 'Overall_Experience' columns if they are duplicates\n",
    "train_merged = train_merged.loc[:,~train_merged.columns.duplicated()]\n",
    "\n",
    "# Display the merged data\n",
    "print(train_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26205b-c403-4f51-bacd-ee4dd8753730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Target Variable and Features\n",
    "\n",
    "# Define features and target\n",
    "X_train = train_merged.drop(columns=['ID', 'Overall_Experience'])  # Drop 'ID' and target 'Overall_Experience'\n",
    "y_train = train_merged['Overall_Experience']  # Target column\n",
    "\n",
    "# Check the shapes of the features and target\n",
    "print(f\"Features shape: {X_train.shape}\")\n",
    "print(f\"Target shape: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b245aca-cc24-4d92-8e02-40d979049116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure the merged train dataset has 'Overall_Experience' as the target column\n",
    "# It seems there were duplicate or renamed columns like 'Overall_Experience_x' and 'Overall_Experience_y'\n",
    "\n",
    "train_merged = train_merged.rename(columns={'Overall_Experience_x': 'Overall_Experience'})\n",
    "train_merged.drop(columns=['Overall_Experience_y'], inplace=True)  # Drop if redundant\n",
    "\n",
    "# Define the target variable and features\n",
    "X_train = train_merged.drop(columns=['ID', 'Overall_Experience'])  # Drop 'ID' and target 'Overall_Experience'\n",
    "y_train = train_merged['Overall_Experience']  # Target column\n",
    "\n",
    "# Check if 'Overall_Experience' is present and correctly handled\n",
    "print(f\"Target column (Overall_Experience) count: {y_train.isnull().sum()}\")\n",
    "\n",
    "# Impute missing values for both numeric and categorical columns in the training data\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Create imputers for numeric and categorical columns\n",
    "numeric_imputer = SimpleImputer(strategy='median')  # For numeric columns\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')  # For categorical columns\n",
    "\n",
    "# Apply imputer for numeric columns in the training data\n",
    "X_train[numeric_cols] = numeric_imputer.fit_transform(X_train[numeric_cols])\n",
    "\n",
    "# Apply imputer for categorical columns in the training data\n",
    "X_train[categorical_cols] = categorical_imputer.fit_transform(X_train[categorical_cols])\n",
    "\n",
    "# Check if missing values are handled in the training set\n",
    "print(f\"Missing values in Training Data after imputation:\\n{X_train.isnull().sum()}\")\n",
    "\n",
    "# Apply imputers for the test data (ensure the same columns are used)\n",
    "test_data[numeric_cols] = numeric_imputer.transform(test_data[numeric_cols])\n",
    "test_data[categorical_cols] = categorical_imputer.transform(test_data[categorical_cols])\n",
    "\n",
    "# Check if missing values are handled in the test set\n",
    "print(f\"Missing values in Test Data after imputation:\\n{test_data.isnull().sum()}\")\n",
    "\n",
    "# Apply OneHotEncoder for categorical columns with nominal categories (e.g., service ratings)\n",
    "categorical_columns = ['Platform_Location', 'Onboard_Wifi_Service', 'Onboard_Entertainment',\n",
    "                       'Online_Support', 'Ease_of_Online_Booking', 'Onboard_Service', 'Legroom',\n",
    "                       'Baggage_Handling', 'CheckIn_Service', 'Cleanliness', 'Online_Boarding']\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, drop='first')  # Drop first to avoid multicollinearity\n",
    "\n",
    "# Create a ColumnTransformer to apply one-hot encoding to categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('cat', one_hot_encoder, categorical_columns)],\n",
    "    remainder='passthrough'  # Keep the rest of the columns as is\n",
    ")\n",
    "\n",
    "# Apply the transformations\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Now the data is encoded and imputed\n",
    "print(f\"Shape of encoded training data: {X_train_encoded.shape}\")\n",
    "\n",
    "# Train XGBoost Model\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Make predictions on the training data\n",
    "y_pred_train = xgb_model.predict(X_train_encoded)\n",
    "\n",
    "# Evaluate the model using accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "print(f\"Training Accuracy (XGBoost): {train_accuracy:.4f}\")\n",
    "\n",
    "# Prepare and preprocess the test data in the same way as training data\n",
    "X_test = test_data.drop(columns=['ID'])\n",
    "\n",
    "# Apply the same imputers to the test data\n",
    "X_test[numeric_cols] = numeric_imputer.transform(X_test[numeric_cols])\n",
    "X_test[categorical_cols] = categorical_imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# Encode the categorical columns in the test data\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_test = xgb_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluate the model on the test set if y_test is available\n",
    "# For now, we are only predicting, so we can save the results\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data['ID'],\n",
    "    'Overall_Experience': y_pred_test\n",
    "})\n",
    "\n",
    "# Save the submission file as CSV\n",
    "submission.to_csv('xgboost_submission.csv', index=False)\n",
    "print(\"Submission file created successfully!\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31da4d59-b81e-4c98-9836-b78d5dad4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning (if needed)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for tuning XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with XGBoost\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "\n",
    "# Fit the model to training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from grid search\n",
    "print(\"Best Hyperparameters (XGBoost):\", grid_search.best_params_)\n",
    "\n",
    "# Train with the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred_best = best_xgb_model.predict(X_train)\n",
    "\n",
    "# Evaluate with the best model\n",
    "best_train_accuracy = accuracy_score(y_train, y_pred_best)\n",
    "print(f\"Best Training Accuracy: {best_train_accuracy:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf2e9a-3a45-4068-ae13-e1a40eae74d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the Model on the Test Data\n",
    "\n",
    "# Merge the test data (apply the same preprocessing steps as training data)\n",
    "test_data_merged = pd.merge(test_data, survey_test, on='ID')\n",
    "\n",
    "# Apply similar preprocessing steps to the test data\n",
    "test_data_merged[numerical_columns] = scaler.transform(test_data_merged[numerical_columns])\n",
    "test_data_merged[categorical_cols] = label_encoder.transform(test_data_merged[categorical_cols])\n",
    "\n",
    "# Define X_test (drop 'ID' and 'Overall_Experience' from test data)\n",
    "X_test = test_data_merged.drop(columns=['ID', 'Overall_Experience'])\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test = best_xgb_model.predict(X_test)\n",
    "\n",
    "# Create the submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_data_merged['ID'],  # Use the ID column from the test set\n",
    "    'Overall_Experience': y_pred_test  # Predicted values for Overall_Experience\n",
    "})\n",
    "\n",
    "# Save the submission file as CSV\n",
    "submission.to_csv('submission_xgb.csv', index=False)\n",
    "print(\"Submission file created successfully!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
